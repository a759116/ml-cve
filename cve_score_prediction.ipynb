{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing and data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create training and test data\n",
    "def simple_split(data,y,length,split_mark=0.7):\n",
    "    if split_mark > 0. and split_mark < 1.0:\n",
    "        n = int(length*split_mark)\n",
    "    else:\n",
    "        n = int(split_mark)\n",
    "    X_train = data[:n].copy()\n",
    "    X_test = data[n:].copy()\n",
    "    y_train = y[:n].copy()\n",
    "    y_test = y[n:].copy()\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = pd.read_csv(\"data/cve.csv\")  \n",
    "#data.head()\n",
    "# split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = simple_split(data.summary,data.cvss,len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depends on cve description analysis, stop words will remove noise\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(['22'])\n",
    "# will convert text to number vector\n",
    "vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "# convert text into word features\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "# print(\"Number of features: {}\".format(len(feature_names)))\n",
    "# print(\"First 20 features: {}\\n\".format(feature_names[:20]))\n",
    "# print(\"Middle 20 features: {}\\n\".format(feature_names[len(feature_names)//2 - 20:len(feature_names)//2]))\n",
    "# print(\"Last 20 features: {}\\n\".format(feature_names[len(feature_names) - 20:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper methods to build and evaluate liner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper methods to build and evaluate liner model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def train_liner_regression_model(X_train, y_train):\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    \n",
    "    return lr_model\n",
    "\n",
    "def evaluate_liner_regression_model(lr_model, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    model_score={}\n",
    "    \n",
    "    y_pred = lr_model.predict(X_test)\n",
    "    \n",
    "    # The mean squared error\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    model_score['mean_squared_error']=round(mse,4)\n",
    "    # print('Mean squared error: %.2f'% mse)\n",
    "    # The root mean squared error\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    model_score['root_mean_squared_error']=round(rmse,4)\n",
    "    # print('Root Mean squared error: %.2f'% rmse)\n",
    "    # The coefficient of determination: 1 is perfect prediction\n",
    "    coef_determination = r2_score(y_test, y_pred)\n",
    "    model_score['coef_determination']=round(coef_determination,4)\n",
    "    # print('Coefficient of determination: %.2f'% coef_determination)\n",
    "    \n",
    "    # R-squared value: R-squared is between 0 and 1, \n",
    "    # Higher values are better because it means that more variance is explained by the model.\n",
    "    #rsq = lr_model.score(X_train, y_train)\n",
    "    rsq = lr_model.score(X_test, y_test)\n",
    "    model_score['R-squared']=round(rsq,4)\n",
    "    # print('R-squared value: %.2f'% rsq)\n",
    "    \n",
    "    return y_pred,model_score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Method for Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(X_train, y_train, X_test, score_func=f_regression, number_of_features='all'):\n",
    "    # configure to select all features\n",
    "    fs = SelectKBest(score_func, k=number_of_features)\n",
    "    # learn relationship from training data\n",
    "    fs.fit(X_train, y_train)\n",
    "    # transform train input data\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    # transform test input data\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    \n",
    "    return X_train_fs, X_test_fs, fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate Linear Regression Model with different set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_for_features(score_func=f_regression, number_of_features='all'):\n",
    "    X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test, score_func,number_of_features)\n",
    "    lr_model = train_liner_regression_model(X_train_fs, y_train)\n",
    "    y_pred, model_score = evaluate_liner_regression_model(lr_model, X_train_fs, y_train, X_test_fs, y_test)\n",
    "    \n",
    "    return lr_model, fs, y_pred, model_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# def load_data(path)\n",
    "# split data\n",
    "# def extract_features_from_text(X_train,X_test,stop_words)\n",
    "\n",
    "\"\"\"\n",
    "CVE_2020_17087=\"A vulnerability exists within the Windows kernel that was publicly disclosed by Google in late October that worked in conjunction with the Chrome bug (CVE-2020-15999) to escape the browser sandbox and execute code. There has been a lot of reporting on these 2 vulnerabilities and we expect them to be weaponized soon.\"\n",
    "\n",
    "CVE_2020_17084 = \"A vulnerability exists within Microsoft Exchange that was demonstrated during a recent conference that bypasses a patch that Microsoft implemented in September (CVE-2020-16875) that enables an attacker to once again achieve Remote Code Execution by exploiting a memory corruption flaw within Exchange. While no POC or Attacks in the Wild exist, the presenter will likely soon make the full details of the vulnerability public soon.\"\n",
    "\"\"\"\n",
    "\n",
    "# Unforseen CVEs: will be used for score prediction\n",
    "nvd_last_20_cves = pd.read_csv(\"data/nvd_last_20_cves.csv\")\n",
    "\n",
    "model_score_list = []\n",
    "model_selected_features = {}\n",
    "\n",
    "for i in range (0, 6):\n",
    "    number_of_features=i*100\n",
    "    if number_of_features == 0:\n",
    "        lr_model,fs,y_pred,model_score = eval_model_for_features(score_func=f_regression,\n",
    "                                                                 number_of_features='all')\n",
    "        model_score['number_of_features'] = 'all'\n",
    "        print(\"Number of features: All \\n\")\n",
    "    else:\n",
    "        lr_model,fs,y_pred,model_score = eval_model_for_features(score_func=f_regression,\n",
    "                                                             number_of_features=number_of_features)\n",
    "        model_score['number_of_features'] = number_of_features\n",
    "        model_selected_features[str(number_of_features)]= fs.get_support(True)\n",
    "        # print(\"Number of features: {}\\n\".format(number_of_features))\n",
    "    \n",
    "    # predict socres for unforeseen vulnerabilities\n",
    "    for index, row in nvd_last_20_cves.iterrows():\n",
    "        # print(row['cve_id'],row['summary'])\n",
    "        model_score[row['cve_id']+'_predict']=round(lr_model.predict(fs.transform(\n",
    "            vectorizer.transform([row['summary']])))[0],4)\n",
    "        model_score[row['cve_id']+'_v2_actual']=row['cvss_score_20']\n",
    "        model_score[row['cve_id']+'_v3_actual']=row['cvss_score_31']\n",
    "        model_score[row['cve_id']+'_v2_diff']=model_score[row['cve_id']+'_predict']-row['cvss_score_20']\n",
    "        model_score[row['cve_id']+'_v3_diff']=model_score[row['cve_id']+'_predict']-row['cvss_score_31']\n",
    "    \n",
    "    model_score_list.append(model_score)\n",
    "    \n",
    "    \n",
    "    # print(\"Scatter Plot: Actual vs. Prediction\")\n",
    "    #sns.scatterplot(x=y_test,y=y_pred)\n",
    "    # plt.scatter(y_test, y_pred, color='red')\n",
    "    # plt.show()\n",
    "\n",
    "model_score_df = pd.DataFrame.from_dict(model_score_list)\n",
    "model_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feature_list = {}\n",
    "for key, values in model_selected_features.items():\n",
    "    # print(\"{} features selected \\n\".format(key))\n",
    "    feature_list = []\n",
    "    for val in values:\n",
    "        feature_list.append(feature_names[val])\n",
    "    model_feature_list[key]=feature_list\n",
    "    # print(\"{} features list length \\n\".format(len(feature_list)))\n",
    "model_feature_df = pd.DataFrame.from_dict(model_feature_list,orient='index').transpose()\n",
    "model_feature_df.to_csv(\"selected_features_f.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_score_df.to_csv(\"model_scores_f.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2_diff_cols=[col for col in model_score_df.columns if 'v2_diff' in col]\n",
    "v2_diff_cols.append('number_of_features')\n",
    "model_v2_diff=model_score_df[v2_diff_cols]\n",
    "model_v2_diff.set_index('number_of_features',inplace=True)\n",
    "#model_v2_diff\n",
    "for index, row in model_v2_diff.iterrows():\n",
    "    print(\"For {} features the difference between predicted and actual V2 scores\\n{}\\n{}\\n\".format(index,\n",
    "                row.describe(),\n",
    "                row.plot(kind=\"hist\")))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v3_diff_cols=[col for col in model_score_df.columns if 'v3_diff' in col]\n",
    "v3_diff_cols.append('number_of_features')\n",
    "model_v3_diff=model_score_df[v3_diff_cols]\n",
    "model_v3_diff.set_index('number_of_features',inplace=True)\n",
    "#model_v3_diff\n",
    "for index, row in model_v3_diff.iterrows():\n",
    "    print(\"For {} features the difference between predicted and actual V3 scores\\n{}\\n{}\\n\".format(index,\n",
    "                row.describe(),\n",
    "                row.plot(kind=\"hist\")))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred=model_score_df[model_score_df['number_of_features']==300]\n",
    "pred_cols=[col for col in model_score_df.columns if 'CVE' in col]\n",
    "model_pred=model_pred[pred_cols]\n",
    "diff_cols=[col for col in model_score_df.columns if 'diff' in col]\n",
    "model_pred=model_pred.drop(diff_cols, axis=1)\n",
    "model_pred=model_pred.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred.to_csv(\"model_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
